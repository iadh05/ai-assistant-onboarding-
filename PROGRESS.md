# Learning Progress - Onboarding Chatbot

## Session: 2025-11-25

### ‚úÖ Completed (Sprint 1 + Sprint 2 Partial)

**Sprint 1 - Polish & UX:**
- [x] S1-1: Fixed vector store reload performance (added `reloadIfChanged()` with mtime tracking)
- [x] S1-2: Added sources display in chat UI (collapsible component)
- [x] S1-3: Integrated conversation storage with localStorage
- [x] S1-4: Made Copy button work with Clipboard API + visual feedback
- [x] S1-5: Verified timeout exists (30s), improved error messages

**Sprint 2 - Caching & Error Handling (COMPLETE):**
- [x] S2-1: Added QueryCache for instant repeat responses
- [x] S2-2: Added EmbeddingCache integrated into VectorStore
- [x] S2-3: Retry logic with exponential backoff (centralized in queryClient)
- [x] S2-4: Friendly error messages (error-messages.ts + gateway error handler)
- [~] S2-5: Cache stats in health check (SKIPPED ‚Üí moved to Sprint 5)

**Deep Dive: Semantic Caching**
- [x] Created semantic caching risk test (`test-semantic-risk.ts`)
- [x] Ran tests showing dangerous false positives (enable vs disable dark mode: 93.2% similarity!)
- [x] Added Sprint 11 to planning: Vector DB & Semantic Caching

**Infrastructure:**
- [x] Fixed dev script to auto-rebuild core/llm packages
- [x] Added mentor rule: "ONE TICKET AT A TIME"
- [x] Added Gemini API configuration to .env

---

## Session: 2025-11-22

### ‚úÖ Completed
- [x] Project setup (Turborepo, TypeScript, packages structure)
- [x] Understanding: What are embeddings?
- [x] Built embedding service (now refactored to LLM package)
- [x] Understanding chunking
- [x] Built chunking service
- [x] Understanding vector store
- [x] Built vector store with embedding integration
- [x] Understanding RAG pattern
- [x] Built chat service with RAG implementation
- [x] **MAJOR REFACTORING: Separated LLM package**
  - Created `packages/llm` with provider pattern
  - XML-based prompt formatting
  - Clean separation of concerns
  - Easy to swap LLM providers
- [x] Organized test files into `tests/` directory
- [x] Added comprehensive .gitignore
- [x] **FULL RAG PIPELINE WORKING!**
- [x] **Built gRPC API Server** (Learning: client-server architecture)
  - Created Protocol Buffer schema
  - Implemented gRPC server with RAG endpoints
  - Tested end-to-end gRPC communication
  - Documented gRPC vs REST differences
- [x] **Promoted Proto to Shared Contracts Package**
  - Created `packages/contracts/` for API contracts
  - Industry-standard separation of interface vs implementation
  - Ready for multiple clients (CLI, web, mobile)
- [x] **MICROSERVICES ARCHITECTURE WITH gRPC GATEWAY!**
  - Refactored monolithic service into 3 backend microservices
  - Built gRPC-to-gRPC gateway (single entry point for clients)
  - Client connects to ONE address, gateway routes to backends
  - Learned microservices patterns and service discovery

### üîÑ Next Steps
- [ ] Build CLI interface with React Ink
- [ ] Add real documentation files
- [ ] Implement document indexing workflow
- [ ] Add conversation history
- [ ] Deploy as CLI tool

---

## Key Learnings

### Caching Strategies (NEW - Session 2025-11-25)

**Two-Layer Cache Architecture:**
```
User Query
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Layer 1: QueryCache (exact match)               ‚îÇ
‚îÇ   Key: normalized question hash                 ‚îÇ
‚îÇ   Value: full ChatResponse {answer, sources}    ‚îÇ
‚îÇ   Hit = instant response (skip everything!)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì (cache miss)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Layer 2: EmbeddingCache (in VectorStore)        ‚îÇ
‚îÇ   Key: text hash                                ‚îÇ
‚îÇ   Value: embedding vector [768 numbers]         ‚îÇ
‚îÇ   Hit = skip Ollama API call (~100-300ms saved) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì (cache miss)
    Generate embedding ‚Üí Search ‚Üí RAG ‚Üí LLM
```

**LRU Cache Pattern:**
- **LRU** = Least Recently Used (evicts oldest unused items)
- **TTL** = Time To Live (auto-expire stale data)
- **Hit Rate** = cache hits / total requests (measure effectiveness)
- Used by Redis, Memcached, and every production system

**Why Two Separate Caches?**
| Cache | Level | What it saves | When it helps |
|-------|-------|---------------|---------------|
| QueryCache | High | Full RAG pipeline | Exact same question |
| EmbeddingCache | Low | Embedding API call | Same text needs embedding |

### Semantic Caching Risk (NEW - Deep Dive)

**The Danger:** High embedding similarity ‚â† Same answer needed!

**Real Test Results:**
```
"How to enable dark mode?" vs "How to disable dark mode?"
Similarity: 93.2%  ‚Üí  ‚ùå WRONG CACHE HIT with 0.90 threshold!
```

**Why This Happens:**
- Embeddings capture "what the text is about" (semantic meaning)
- Both questions are "about dark mode" and "about how-to"
- But the **intent is opposite** (enable vs disable)

**Safe Production Approaches:**
1. **Conservative Threshold (0.95+)** - Miss opportunities, never wrong
2. **LLM Verification** - Ask small LLM "Are these the same question?"
3. **Hybrid** - High threshold + keyword checking (enable/disable, add/remove)

**Our Choice:** Exact-match QueryCache only (safest for onboarding docs)

### File Modification Time (mtime) Pattern

**Problem:** VectorStore was reloading from disk on every query (slow!)

**Solution:** Track file's `mtimeMs` (modification timestamp)
```typescript
if (stats.mtimeMs > this.lastModifiedTime) {
  await this.load();  // File changed, reload
}
// else: skip reload, use cached data
```

**Where This Pattern is Used:**
- Build systems (webpack, tsc watch mode)
- File sync tools (Dropbox, rsync)
- Hot reload in development servers
- Our VectorStore!

### Embeddings
- **Purpose:** Convert text to numbers to compare meaning
- **Model:** nomic-embed-text (via Ollama)
- **Output:** 768 numbers per text
- **Use case:** Find similar docs by comparing number arrays
- **Architecture:** Now uses `EmbeddingProvider` interface for flexibility

### Cosine Similarity
- **Purpose:** Measure how similar two embeddings are
- **Formula:** dotProduct / (magnitude1 √ó magnitude2)
- **Score:** 0.0 (different) to 1.0 (identical)
- **How it works:**
  1. Dot product: Multiply matching positions, sum them
  2. Magnitude: Calculate "length" of each vector
  3. Divide: Normalizes for fair comparison
- **Visual:** Measures the "angle" between vectors (small angle = similar)
- **Architecture:** Isolated in `utils/similarity.ts` (will be removed when using vector DB)

### Chunking
- **Purpose:** Split large documents into smaller, meaningful pieces
- **Strategy:** Hybrid approach - split by markdown headings, with max size fallback
- **Why needed:** LLMs have token limits, embeddings work better on focused chunks
- **Implementation:** `ChunkingService` with heading-based splitting
- **Metadata:** Tracks source, heading, and index for each chunk

### Vector Store
- **Purpose:** Store document chunks with their embeddings for fast similarity search
- **Current implementation:** JSON file (simple, works for POC)
- **Architecture:** Uses `EmbeddingProvider` interface (easy to swap providers)
- **Key methods:**
  - `addChunks()` - generates embeddings and stores chunks
  - `search()` - finds most similar chunks to a query
  - `save()/load()` - persistence
- **Future:** Will upgrade to real vector database (Pinecone, Weaviate, Chroma)

### RAG (Retrieval Augmented Generation)
- **Purpose:** Give LLM accurate, grounded answers using your own documentation
- **Three steps:**
  1. **Retrieval:** Search vector store for relevant chunks
  2. **Augmentation:** Build XML prompt with retrieved context
  3. **Generation:** LLM generates answer based on context
- **Why it works:**
  - LLM doesn't hallucinate (uses only provided docs)
  - Easy to update knowledge (just update docs)
  - Shows sources used
  - Admits when it doesn't know

### Provider Pattern (Architecture)
- **Purpose:** Separate business logic from infrastructure
- **Benefits:**
  - Easy to swap LLMs (Ollama ‚Üí OpenAI ‚Üí Claude)
  - Easy to test (use mock providers)
  - Multi-model support (route based on complexity)
  - Cost optimization
- **Implementation:**
  - `LLMProvider` interface for chat generation
  - `EmbeddingProvider` interface for embeddings
  - Ollama implementations in `packages/llm/providers/ollama/`

### XML Prompts
- **Purpose:** Structured, unambiguous prompts for LLMs
- **Benefits:**
  - Clear section boundaries (`<system>`, `<documents>`, `<instructions>`, `<question>`)
  - Easier for LLM to parse
  - Industry standard (Anthropic uses heavily)
  - Easy to modify programmatically
- **Implementation:** `PromptBuilder` class with `buildRAGPrompt()`

### gRPC (Client-Server Communication)
- **Purpose:** Learn how to build APIs for microservices (vs Express for web apps)
- **Key Concepts:**
  - **Protocol Buffers (.proto):** Strict API contract, auto-generated code
  - **Binary Format:** Faster and smaller than JSON (40-60% reduction)
  - **HTTP/2:** Multiplexing, streaming, better performance
  - **Type Safety:** Compile-time checking, impossible to get out of sync
- **When to use:**
  - gRPC: Microservices, high-performance, server-to-server
  - REST: Web apps, public APIs, browser compatibility
  - In-Process: CLI tools, desktop apps (simplest)
- **Implementation:**
  - Service definition in `packages/contracts/proto/chat.proto`
  - Backend services in `packages/api/src/services/`
  - Gateway in `packages/api/src/gateway/grpc-gateway.ts`
  - Client test in `packages/api/src/test-gateway.ts`
- **See:** [GRPC_VS_REST.md](GRPC_VS_REST.md) for detailed comparison

### Microservices Architecture
- **Purpose:** Learn distributed system design and service-oriented architecture
- **Pattern:** API Gateway (single entry point) + Backend Services
- **Key Concepts:**
  - **Service Separation:** Each service has single responsibility
    - Document Service (port 50051): Document management, chunking, vector storage
    - Chat Service (port 50052): RAG query processing, LLM interaction
    - System Service (port 50053): Health checks, system info
  - **API Gateway (port 8080):** Single entry point for all clients
    - Client doesn't know about backend services
    - Gateway proxies requests to appropriate backend
    - Simplifies client code (one connection vs. three)
  - **Service Discovery:** Gateway knows backend service addresses
  - **Protocol Buffers:** Changed from `chatbot` to `onboarding.chatbot.v1` (versioning)
- **Benefits:**
  - **Independent Scaling:** Scale only the services you need
  - **Independent Deployment:** Update one service without touching others
  - **Technology Flexibility:** Each service can use different tech stack
  - **Team Autonomy:** Different teams can own different services
  - **Fault Isolation:** One service failing doesn't crash everything
- **Implementation:**
  - Proto refactored into 3 separate services (DocumentService, ChatService, SystemService)
  - Each service runs independently on different port
  - Gateway creates gRPC clients to all backend services
  - Gateway implements all service interfaces and proxies calls
  - Client connects only to gateway, not backend services
- **When to use:**
  - Large applications with multiple concerns
  - Teams working independently
  - Need to scale different parts differently
  - Learning distributed systems
- **Trade-offs:**
  - More complexity (multiple processes, network calls)
  - Need for monitoring and logging
  - Harder to debug (distributed tracing needed)
  - Local development requires running multiple services

---

## Test Results

### Embedding Test
‚úÖ Successfully generated embeddings and compared similarity
- "Hello" vs "Hi": 0.879 (high similarity - similar meaning)
- "Hello" vs "Installing Node.js": 0.334 (low similarity - different meaning)

### Chunking Test
‚úÖ Successfully split document into 4 chunks by markdown headings
- Each chunk has proper metadata (source, heading, index)

### Vector Store Test
‚úÖ Successfully created vector store with semantic search
- Query: "How do I install Node?" ‚Üí Found "Installation" (score: 0.847)
- Query: "How to create a project?" ‚Üí Found "First Project" (score: 0.680)

### Full RAG Chat Test
‚úÖ **END-TO-END RAG PIPELINE WORKING!**

**Test 1:** "How do I install Node.js?"
- Retrieved: Installation docs (score: 0.864)
- Answer: Correctly synthesized steps from documentation
- Sources: Showed which docs were used

**Test 2:** "What should I do if npm install fails?"
- Retrieved: Troubleshooting section (score: 0.717)
- Answer: Provided correct solution from docs
- Sources: Referenced troubleshooting guide

**Test 3:** "How do I deploy to AWS?"
- Retrieved: Best matching docs (score: 0.579)
- Answer: **Correctly admitted it doesn't have that information!**
- No hallucination - this is the key benefit of RAG!

### gRPC API Test
‚úÖ **CLIENT-SERVER ARCHITECTURE WORKING!**

**Test 1:** Health Check
- ‚úì Server responds with status, LLM model, and embedding model
- ‚úì gRPC connection established successfully

**Test 2:** Add Document via API
- ‚úì Added 1 document over gRPC
- ‚úì Server chunked it into 3 pieces
- ‚úì Vector embeddings generated remotely

**Test 3:** Ask Question via gRPC
- Question: "How do I install Node.js?"
- ‚úì Retrieved 3 relevant chunks
- ‚úì LLM generated answer using retrieved context
- ‚úì Sources returned with answer
- **Key Learning:** Same RAG logic, but over network instead of in-process!

### Microservices Gateway Test
‚úÖ **MICROSERVICES ARCHITECTURE WORKING VIA GATEWAY!**

**Architecture:**
- Client connects to: `localhost:8080` (Gateway)
- Gateway routes to:
  - Document Service (`localhost:50051`)
  - Chat Service (`localhost:50052`)
  - System Service (`localhost:50053`)

**Test 1:** Health Check via Gateway
- ‚úì Client ‚Üí Gateway (8080) ‚Üí System Service (50053)
- ‚úì Gateway logged: `[Gateway] Proxying HealthCheck to System Service`
- ‚úì Response: Status OK, llama3.2, nomic-embed-text

**Test 2:** Add Document via Gateway
- ‚úì Client ‚Üí Gateway (8080) ‚Üí Document Service (50051)
- ‚úì Gateway logged: `[Gateway] Proxying AddDocuments to Document Service`
- ‚úì Added 1 document, chunked into 3 pieces

**Test 3:** Ask Question via Gateway
- Question: "What are microservices?"
- ‚úì Client ‚Üí Gateway (8080) ‚Üí Chat Service (50052)
- ‚úì Gateway logged: `[Gateway] Proxying AskQuestion to Chat Service`
- ‚úì Retrieved relevant chunks and generated answer

**Key Learnings:**
- Client code is simpler - one connection point instead of three
- Gateway handles service discovery and routing
- Backend services are hidden from clients
- Easy to add load balancing, authentication, rate limiting at gateway level
- Logs show clear routing: which service handled which request

---

## Architecture Achievements

### Clean Separation of Concerns
```
packages/contracts/         ‚Üí API contracts (Protocol Buffers)
packages/llm/               ‚Üí Infrastructure (LLM providers)
packages/core/              ‚Üí Business logic (RAG, chunking, vector store)
packages/api/
  ‚îú‚îÄ‚îÄ src/services/         ‚Üí Backend microservices (document, chat, system)
  ‚îú‚îÄ‚îÄ src/gateway/          ‚Üí API Gateway (single entry point)
  ‚îî‚îÄ‚îÄ src/test-gateway.ts   ‚Üí Gateway client test
packages/cli/               ‚Üí User interface (to be built)
```

**Key insights:**
- The `.proto` contract is separate from implementation, allowing multiple clients to share the same contract
- Microservices architecture separates concerns by service boundary
- Gateway pattern provides single entry point while maintaining service independence

### Provider Pattern Implementation
- Interfaces define contracts
- Implementations are swappable
- Easy to add new providers (OpenAI, Claude, etc.)
- Tests can use mock providers

### Organized Structure
- Tests in dedicated `tests/` folder
- Providers organized by vendor (`ollama/`, future: `openai/`, `anthropic/`)
- Clean imports and dependencies
- Proper TypeScript declarations

---

## üìä Session Assessment: 2025-11-25

### Skills Demonstrated

| Skill | Level | Evidence |
|-------|-------|----------|
| **Caching Patterns** | ‚≠ê‚≠ê‚≠ê‚≠ê Strong | Built LRU cache, understands TTL, hit rates |
| **System Design** | ‚≠ê‚≠ê‚≠ê‚≠ê Strong | Two-layer cache architecture, mtime optimization |
| **Critical Thinking** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellent | Asked the RIGHT question about semantic caching risk |
| **Testing Mindset** | ‚≠ê‚≠ê‚≠ê‚≠ê Strong | Wanted to see real test data before trusting theory |
| **Learning Initiative** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellent | Paused sprint to deep-dive on interesting topic |

### Key Moment of the Session

**You asked:** "What happens if the user sends a query that's not similar but approximately similar?"

This is a **senior engineer question**. Most developers blindly implement semantic caching because it sounds cool. You questioned the assumption and discovered a real production risk.

### What You Learned Today

1. **LRU Cache** - Industry-standard eviction pattern
2. **TTL (Time To Live)** - Auto-expiring stale data
3. **Two-Layer Caching** - QueryCache (high) + EmbeddingCache (low)
4. **mtime Pattern** - Skip unnecessary disk reads
5. **Semantic Caching Risk** - High similarity ‚â† Same answer
6. **False Positives** - The danger of trusting embeddings blindly

### Progress Toward Senior Engineer

```
Mid-Level                                      Senior
   |==========================================>|
   ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë
                      ‚ñ≤
                  You are here

Key indicator: Questioning assumptions, not just implementing
```

### Sprint 2 COMPLETE!

All tasks done except S2-5 (Cache stats in health check) which was intentionally moved to Sprint 5.

**Session 2025-11-26 Additions:**
- [x] S2-3: Retry logic - centralized in `queryClient.ts` (queries + mutations)
- [x] S2-4: Error handling layer - separate `gateway/errors/` module with gRPC status code mapping
- [x] CORS configuration with environment variable
- [x] Transport documentation (`docs/transport-and-sockets.md`)

### Files Created/Modified This Session

**New Files:**
- `packages/core/src/cache/lru-cache.ts` - Generic LRU cache
- `packages/core/src/cache/query-cache.ts` - Chat response cache
- `packages/core/src/cache/embedding-cache.ts` - Embedding vector cache
- `packages/core/src/cache/index.ts` - Cache exports
- `packages/core/src/tests/test-semantic-risk.ts` - Semantic caching risk demo

**Modified Files:**
- `packages/core/src/vectorStore.ts` - Added EmbeddingCache + reloadIfChanged
- `packages/core/src/chat.ts` - Added QueryCache integration
- `packages/core/src/index.ts` - Export caches
- `packages/web/src/modules/chat/views/chat.tsx` - Sources, copy button, localStorage
- `package.json` - Dev script with watchers
- `CLAUDE.md` - Added "ONE TICKET AT A TIME" rule
- `SPRINT-PLANNING.md` - Added Sprint 11 (Vector DB & Semantic Caching)
- `.env` / `.env.example` - Gemini configuration

### Next Session Recommendation

**Ready for Sprint 3: Document Ingestion & Validation**
- S3-1: Accept file uploads (PDF, DOCX, TXT, MD)
- S3-2: Validate file types, sizes, content
- S3-3: Sanitize text content
- S3-4: Extract text from PDF
- S3-5: Show upload progress
- S3-6: Document deduplication

This sprint will teach you file handling, validation patterns, and building robust input pipelines - all critical skills for production systems.

---

## Session Assessment: 2025-11-26

### Checkpoint Status

| Sprint | Status | Completion |
|--------|--------|------------|
| Sprint 1 | COMPLETE | 100% |
| Sprint 2 | COMPLETE | 100% (S2-5 moved to Sprint 5) |
| Sprint 3 | NOT STARTED | 0% |

**Overall Project Progress:** ~20% (2 of 10 sprints complete)

### Skills Demonstrated This Session

| Skill | Rating | Evidence |
|-------|--------|----------|
| **Debugging** | Strong | Identified retry wasn't working because mutations weren't configured |
| **Architecture Decisions** | Strong | Chose centralized config over per-hook config (DRY principle) |
| **Separation of Concerns** | Strong | Created dedicated error handler layer for gateway |
| **Asking Questions** | Excellent | Deep-dived on transport agnostic concept until fully understood |
| **Knowing When to Stop** | Excellent | Recognized feeling overwhelmed and requested checkpoint |

### Honest Assessment

**What Went Well:**
- You correctly identified that retry logic needed to be in queryClient, not individual hooks
- You pushed for deeper understanding of transport concepts rather than accepting surface-level explanations
- You requested documentation for complex topics you knew you'd forget - this is professional behavior
- You recognized when you were hitting cognitive overload and asked to pause

**Areas for Improvement:**
- Some concepts like Unix sockets and gRPC status codes may need revisiting - they were covered quickly near the end when you were already tired
- The session was long and covered a lot of ground - consider shorter, more focused sessions

**Key Learnings:**
1. **React Query retry** - Must configure separately for queries AND mutations
2. **gRPC Status Codes** - 0-16, transport agnostic, maps to HTTP
3. **Unix Sockets** - File-based IPC, Linux/Mac only, faster than TCP
4. **Health Checks** - Infrastructure polls them, not UI
5. **Error Handler Layering** - Separate concerns between gateway and services

### Senior Engineer Progress

```
Session Start                              Session End
     |                                          |
     ‚ñº                                          ‚ñº
Mid ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë Senior
                   ‚ñ≤           ‚ñ≤
              Last session   This session
                             (small but solid progress)
```

**Key Indicator:** You're developing the habit of questioning implementations and asking "why does it work this way?" This is the hallmark of a senior engineer mindset.

### What's Different Now vs. Session Start

Before this session:
- Retry logic existed but wasn't working for mutations
- Error handling was inline in routes
- No documentation on transport concepts

After this session:
- Retry works for both queries AND mutations (centralized)
- Dedicated error handler layer with gRPC status code mapping
- CORS properly configured via environment variable
- Transport documentation for future reference

### Recommendation for Next Session

1. **Start Fresh** - Don't try to immediately dive into Sprint 3 when tired
2. **Review Transport Docs** - Quick skim of `docs/transport-and-sockets.md` before starting
3. **Sprint 3 Focus** - File uploads and validation are practical, hands-on work
4. **Shorter Session** - Consider 2-3 tickets per session to avoid overload

---

## Session Assessment: 2025-11-28

### Sprint 3: Document Ingestion & Validation - COMPLETE ‚úÖ

**What Was Built:**

| Component | Location | Purpose |
|-----------|----------|---------|
| Docker + MinIO | `docker-compose.yml` | S3-compatible object storage |
| StorageService | `api/src/services/storage/` | Upload, download, streaming, hashing |
| FileValidator | `core/src/ingestion/FileValidator.ts` | Type, size, magic bytes validation |
| ContentExtractor | `core/src/ingestion/ContentExtractor.ts` | PDF, DOCX, TXT, MD text extraction |
| TextSanitizer | `core/src/ingestion/TextSanitizer.ts` | Clean text for RAG |
| DocumentDeduplicator | `core/src/ingestion/DocumentDeduplicator.ts` | Content hash duplicate detection |
| Upload REST API | `api/src/gateway/routes/documents.ts` | Multipart upload + clear endpoint |
| File Drop Zone UI | `web/src/modules/documents/components/` | Drag & drop + progress tracking |
| Test Dataset | `test-data/` | 7 sample files + generation script |

**Files Created This Session:**
- `packages/api/src/services/storage/` (5 files - refactored from monolith)
- `packages/core/src/ingestion/` (4 files)
- `packages/web/src/modules/documents/components/` (2 files)
- `test-data/` (7 test files + generator + package.json)
- `docker-compose.yml`

### Honest Assessment

**What Went Well:**
- You asked good clarifying questions about streams (`_transform`, `pipe`, who calls what)
- You pushed back on unnecessary complexity (removed text tab, focused on file upload only)
- You thought about future features (technical dictionary/glossary) without implementing them now
- You identified when test files weren't working (PDF parsing issue) and we fixed it

**What Struggled:**
- **Stream concepts took multiple explanations** - `pipe`, `_transform`, consumer-driven flow weren't immediately clear. This is normal - streams are genuinely confusing, but worth revisiting.
- **State synchronization issue** - Old "Return and Refund Policy" docs polluting RAG results. You didn't catch this before testing. A senior engineer would verify clean state before testing.
- **Port confusion** - Tried `localhost:3000` when API is on `8080`. Small thing, but shows need to internalize the architecture.

**What You Learned:**
1. **Node.js Streams** - Transform streams, pipe connections, lazy pull pattern
2. **S3/MinIO Integration** - AWS SDK, streaming uploads, content hashing
3. **File Processing Pipeline** - Validate ‚Üí Store ‚Üí Extract ‚Üí Sanitize ‚Üí Dedup ‚Üí Index
4. **Binary File Handling** - Magic bytes, PDF/DOCX parsing libraries
5. **State Drift Problem** - Multiple stores (MinIO, VectorStore, Dedup) can get out of sync

**What You Should Review:**
- Node.js streams documentation - the concepts came up repeatedly and weren't fully solid
- The full upload pipeline flow - trace a file from drop to RAG query

### Skills Assessment

| Skill | Rating | Notes |
|-------|--------|-------|
| **Asking Questions** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Great stream questions, pushed for clarity |
| **System Design** | ‚≠ê‚≠ê‚≠ê‚≠ê | Good architecture decisions, proper separation |
| **Debugging** | ‚≠ê‚≠ê‚≠ê | Found issues but sometimes needed hints |
| **Stream Concepts** | ‚≠ê‚≠ê‚≠ê | Understood eventually, but needs reinforcement |
| **Testing Mindset** | ‚≠ê‚≠ê‚≠ê | Tested but didn't verify clean state first |
| **Knowing Scope** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Correctly deferred version management to post-MVP |

### Senior Engineer Progress

```
Session Start                                    Session End
     |                                                |
     ‚ñº                                                ‚ñº
Mid ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë Senior
                          ‚ñ≤                    ‚ñ≤
                     Last session         This session
                                    (+file handling, streams,
                                     state sync awareness)
```

**Progress:** Solid. You built a real file upload pipeline with proper validation, extraction, and deduplication. The stream confusion is normal - these are genuinely hard concepts that even experienced developers struggle with.

**Key Growth Indicator:** You're starting to think about data consistency (why old docs were appearing). This is a database/distributed systems concern that separates mid-level from senior engineers.

### Sprint Progress Summary

| Sprint | Status | Completion |
|--------|--------|------------|
| Sprint 1 | ‚úÖ COMPLETE | 100% |
| Sprint 2 | ‚úÖ COMPLETE | 100% |
| Sprint 3 | ‚úÖ COMPLETE | Core features done, version mgmt deferred |
| Sprint 4 | NOT STARTED | 0% |

**Overall Project Progress:** ~30% (3 of 10 sprints complete)

### Next Session: Sprint 4

**Theme:** Query Understanding & Conversation Memory

Focus areas:
- Intent detection (how-to vs troubleshooting vs definition)
- Query expansion (synonyms for better recall)
- Conversation memory (sliding window)

This will teach you NLP basics and context management - critical for making the chatbot actually useful.
